# Inference configuration
inference:
  # Model paths (download from README.md links to data/models/)
  model_paths:
    1h: "data/models/pangu_weather_1.onnx"
    3h: "data/models/pangu_weather_3.onnx"
    6h: "data/models/pangu_weather_6.onnx"
    24h: "data/models/pangu_weather_24.onnx"

  # Input/Output directories
  input_dir: "input_data"
  output_dir: "output_data"

  # Execution provider ('CPUExecutionProvider' or 'CUDAExecutionProvider')
  execution_provider: "CPUExecutionProvider"

  # ONNX Runtime options
  ort_options:
    enable_cpu_mem_arena: false
    enable_mem_pattern: false
    enable_mem_reuse: false
    intra_op_num_threads: 1

  # CUDA provider options (if using GPU)
  cuda_provider_options:
    arena_extend_strategy: "kSameAsRequested"
